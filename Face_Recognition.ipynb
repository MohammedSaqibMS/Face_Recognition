{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4141b469-0bab-4029-8b06-fcd89eec3031",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8270c1-e8ac-4f84-99e5-e566911fda1e",
   "metadata": {},
   "source": [
    "#### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf416bd-4cbf-4bec-8bb9-f474141ec4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 12:47:07.010427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-17 12:47:07.027279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737100027.048011   30678 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737100027.053955   30678 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 12:47:07.074157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2  # For image processing\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Conv2D, ZeroPadding2D, Activation, Input, BatchNormalization, MaxPooling2D,\n",
    "    AveragePooling2D, Concatenate, Lambda, Flatten, Dense\n",
    ")\n",
    "from keras.initializers import glorot_uniform  # Xavier initialization\n",
    "from keras import backend as K\n",
    "from fr_utils import *  # Custom utility functions\n",
    "from inception_blocks_v2 import *  # Custom inception block implementation\n",
    "\n",
    "# Jupyter-specific configuration\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Suppress scientific notation for large arrays\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Set TensorFlow's default image data format (Modern practice uses channels_last by default)\n",
    "K.set_image_data_format('channels_first')  # If required by your pre-trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fc861-52d5-41fb-9f2f-ba91e8f821fa",
   "metadata": {},
   "source": [
    "## 0 - Naive Face Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1244f-21ba-4816-9bf0-cdb2f7b5558b",
   "metadata": {},
   "source": [
    "## 1 - Encoding face images in 128-dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50bef6-8abe-473f-98da-ceb46364d347",
   "metadata": {},
   "source": [
    "### 1.1 - Using a ConvNet to compute encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162cf221-0214-46dd-a8ae-14f4167376f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737100029.777411   30678 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6257 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Initialize the face recognition model with the input shape\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "220d08f0-e775-4f56-b377-02ce1229bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of parameters in the model\n",
    "print(\"Total Params:\", FRmodel.count_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4c33f-444d-45b2-b84d-68b6b2c0dade",
   "metadata": {},
   "source": [
    "### 1.2 The Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b5f6c7-4e21-4ce5-a198-8667b51c3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the triplet loss function\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Computes the triplet loss as per the formula:\n",
    "    L = max(||f(a) - f(p)||^2 - ||f(a) - f(n)||^2 + alpha, 0)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- True labels (not used, required by Keras loss function signature).\n",
    "    y_pred -- A list containing three tensors:\n",
    "              anchor    - Encodings for anchor images, shape (batch_size, embedding_size).\n",
    "              positive  - Encodings for positive images, shape (batch_size, embedding_size).\n",
    "              negative  - Encodings for negative images, shape (batch_size, embedding_size).\n",
    "    alpha -- Margin parameter for the triplet loss (default is 0.2).\n",
    "    \n",
    "    Returns:\n",
    "    loss -- Scalar, the computed triplet loss value.\n",
    "    \"\"\"\n",
    "    # Extract anchor, positive, and negative encodings from predictions\n",
    "    anchor, positive, negative = y_pred\n",
    "\n",
    "    # Compute the squared distances between anchor-positive and anchor-negative\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)  # ||f(a) - f(p)||^2\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)  # ||f(a) - f(n)||^2\n",
    "\n",
    "    # Compute the basic loss: (positive distance - negative distance + alpha)\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "\n",
    "    # Apply the ReLU activation to ensure the loss is non-negative\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))  # max(basic_loss, 0)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c14b2e-76f5-4ce6-b396-89c1afd2e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 527.2598\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Define random tensors for the anchor, positive, and negative encodings\n",
    "y_pred = (\n",
    "    tf.random.normal([3, 128], mean=6, stddev=0.1, seed=1),\n",
    "    tf.random.normal([3, 128], mean=1, stddev=1, seed=1),\n",
    "    tf.random.normal([3, 128], mean=3, stddev=4, seed=1),\n",
    ")\n",
    "\n",
    "# Compute the triplet loss\n",
    "loss = triplet_loss(None, y_pred)\n",
    "\n",
    "# Print the computed loss value\n",
    "print(f\"Loss = {loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2618e8d-5465-4ed0-9c7a-bee96de5e931",
   "metadata": {},
   "source": [
    "## 2 - Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ef889b-7ff9-40eb-84bf-be681ed5ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Face Recognition model\n",
    "# Using Adam optimizer and triplet loss as the loss function\n",
    "FRmodel.compile(\n",
    "    optimizer='adam', \n",
    "    loss=triplet_loss, \n",
    "    metrics=['accuracy']  # Note: Accuracy may not be meaningful for triplet loss\n",
    ")\n",
    "\n",
    "# Load pre-trained weights into the model\n",
    "load_weights_from_FaceNet(FRmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11ba65-0594-42ce-b0a4-91f72943502b",
   "metadata": {},
   "source": [
    "## 3 - Applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ecb52-2283-47ca-b0bd-f01c29a75db6",
   "metadata": {},
   "source": [
    "### 3.1 - Face Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a8dcd3-2e15-47b3-8d30-d405863e82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 96, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737100078.362371   31173 service.cc:148] XLA service 0x791ce80034a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737100078.362414   31173 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2025-01-17 12:47:58.448030: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1737100078.845531   31173 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-17 12:48:00.254242: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n",
      "(1, 3, 96, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737100081.346957   31173 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty database for storing encodings\n",
    "database = {}\n",
    "\n",
    "# List of user names and corresponding image file paths\n",
    "user_images = {\n",
    "    \"danielle\": \"images/danielle.png\",\n",
    "    \"younes\": \"images/younes.jpg\",\n",
    "    \"tian\": \"images/tian.jpg\",\n",
    "    \"andrew\": \"images/andrew.jpg\",\n",
    "    \"kian\": \"images/kian.jpg\",\n",
    "    \"dan\": \"images/dan.jpg\",\n",
    "    \"sebastiano\": \"images/sebastiano.jpg\",\n",
    "    \"bertrand\": \"images/bertrand.jpg\",\n",
    "    \"kevin\": \"images/kevin.jpg\",\n",
    "    \"felix\": \"images/felix.jpg\",\n",
    "    \"benoit\": \"images/benoit.jpg\",\n",
    "    \"arnaud\": \"images/arnaud.jpg\",\n",
    "}\n",
    "\n",
    "# Populate the database with image encodings\n",
    "for name, image_path in user_images.items():\n",
    "    try:\n",
    "        # Encode the image using the FRmodel\n",
    "        database[name] = img_to_encoding(image_path, FRmodel)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file '{image_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image for {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b548cf-f89e-4127-8f15-8c6d3ba5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path: str, identity: str, database: dict, model) -> tuple:\n",
    "    \"\"\"\n",
    "    Verifies if the person in the provided image matches the specified identity.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - identity (str): Name of the person to verify. Must exist in the database.\n",
    "    - database (dict): Mapping of names to their respective encodings (vectors).\n",
    "    - model: Inception model instance for encoding.\n",
    "\n",
    "    Returns:\n",
    "    - dist (float): The computed distance between the input image encoding and the stored encoding for the identity.\n",
    "    - door_open (bool): True if the door should open, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Compute the encoding for the input image\n",
    "        encoding = img_to_encoding(image_path, model)\n",
    "\n",
    "        # Step 2: Calculate the Euclidean distance between the input encoding and the database encoding for the identity\n",
    "        dist = np.linalg.norm(encoding - database[identity])\n",
    "\n",
    "        # Step 3: Determine if the door should open based on the distance threshold\n",
    "        if dist < 0.7:\n",
    "            print(f\"It's {identity}, welcome in!\")\n",
    "            door_open = True\n",
    "        else:\n",
    "            print(f\"It's not {identity}, please go away.\")\n",
    "            door_open = False\n",
    "\n",
    "    except KeyError:\n",
    "        # Handle case where the identity is not in the database\n",
    "        print(f\"Error: Identity '{identity}' not found in the database.\")\n",
    "        return None, False\n",
    "    except FileNotFoundError:\n",
    "        # Handle missing image file\n",
    "        print(f\"Error: Image file '{image_path}' not found.\")\n",
    "        return None, False\n",
    "    except Exception as e:\n",
    "        # Catch all other exceptions\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None, False\n",
    "\n",
    "    return dist, door_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8d91b9-8e32-409f-b507-6de477993587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 96, 96)\n",
      "It's younes, welcome in!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.659367, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if \"camera_0.jpg\" matches \"younes\" in the database\n",
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9844220-61c5-463d-888c-c35fa7d5f79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 96, 96)\n",
      "It's not kian, please go away.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8622286, False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify if \"kian\" matches the face in \"images/camera_2.jpg\" using the FRmodel and database.\n",
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa936c-bcbb-47d5-8466-e3b1318d1150",
   "metadata": {},
   "source": [
    "### 3.2 - Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19d0bbf4-4f82-4a48-8616-3ccfd3195d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Identifies the person in the given image by finding the closest match in the database.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- str: Path to the image for identification.\n",
    "    database -- dict: A dictionary with names as keys and image encodings as values.\n",
    "    model -- Keras model: Pre-trained Inception model for generating image encodings.\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- float: Minimum distance between the input image encoding and the database encodings.\n",
    "    identity -- str: Name of the identified person from the database, or None if not found.\n",
    "    \"\"\"\n",
    "    # Compute the encoding for the input image.\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "\n",
    "    # Initialize minimum distance to a high value and identity to None.\n",
    "    min_dist = float('inf')  # Use infinity for clarity in comparisons.\n",
    "    identity = None\n",
    "\n",
    "    # Iterate through the database to find the closest match.\n",
    "    for name, db_enc in database.items():\n",
    "        # Compute the L2 (Euclidean) distance between encodings.\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        # Update the minimum distance and identity if a closer match is found.\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    # Print the result based on the threshold value.\n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")  # No match found.\n",
    "    else:\n",
    "        print(f\"It's {identity}, the distance is {min_dist:.2f}\")  # Match found.\n",
    "\n",
    "    return min_dist, identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce32941a-7f8b-4dad-8133-8f20dbdc162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 96, 96)\n",
      "It's younes, the distance is 0.66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.659367, 'younes')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the person in the given image using the face recognition model.\n",
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0517f-0f98-44cd-aba7-356a3a410673",
   "metadata": {},
   "source": [
    "### Future work\n",
    "#### Ways to improve your facial recognition model\n",
    "Although we won't implement it here, here are some ways to further improve the algorithm:\n",
    "- Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.\n",
    "- Crop the images to just contain the face, and less of the \"border\" region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0f6ea-004e-424b-9b0a-a81addeb8dba",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db66223-751b-4b32-800a-4f37a5f31e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
